# autocode

Research on technology that automatically generates high-quality source code from requirements written in natural language, execution examples, or partially written source code.

>-  Automatic source code generation technology that combines new and existing techniques such as machine learning (language model), program synthesis, and software engineering.

![image](./overview_autocode.png)

# Relate References

>- Copilot https://github.com/features/copilot
  
>- Alphacode https://www.deepmind.com/blog/competitive-programming-with-alphacode

>- Paper list
>
    •	"Program synthesis using natural language" , ICSE 2016 , https://dl.acm.org/doi/10.1145/2884781.2884786

    •	"Learning Syntactic Program Transformations from Examples", ICSE 2017, https://ieeexplore.ieee.org/document/7985680

    •	"Synthesis and machine learning for heterogeneous extraction", PLDI 2019. https://dl.acm.org/doi/10.1145/3314221.3322485

    •	"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis", OOPSLA 2021
    
    •	"Semantic programming by example with pre-trained models", OOPSLA 2021, https://www.microsoft.com/en-us/research/publication/semantic-programming-by-example-with-pre-trained-models/
    
    •	"Synchromesh: Reliable Code Generation from Pre-trained Language Models", ICLR 2022,  https://iclr.cc/Conferences/2022/Schedule?showEvent=6709
    
    •	"Overwatch: Learning Patterns in Code Edit Sequences", OOPSLA 2022, https://arxiv.org/abs/2207.12456
    
    •	"Neurosymbolic Repair for Low-Code Formula Languages", OOPSLA 2022, https://arxiv.org/abs/2207.11765
    
    •	"Jigsaw: Large Language Models meet Program Synthesis" , ICSE 2022, https://conf.researchr.org/details/icse-2022/icse-2022-papers/178/Jigsaw-Large-Language-Models-meet-Program-Synthesis

