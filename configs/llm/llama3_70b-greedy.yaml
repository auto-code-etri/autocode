max_tokens: 16384
model: meta-llama/Llama-3.1-70B-Instruct
platform: vllm
temperature: 0.8
top_p: 1
