max_tokens: 16384
model: meta-llama/Llama-3.1-8B-Instruct
platform: vllm
temperature: 0
top_p: 1